---
layout: post
title: ICML 2022 Day 3
author: Rylan Schaeffer
date: 2022-07-19
tags: machine-learning
---

## Tackling covariate shift with node-based Bayesian neural networks
###  Trung Trinh, Markus Heinonen, Luigi Acerbi, Samuel Kaski

- [Slides](https://icml.cc/media/icml-2022/Slides/17368_nXMQMZ0.pdf)
- 


## Why the Rich Get Richer? On the Balancedness of Random Partition Models
### Changwoo Lee, Huiyan Sang 

- [Slides](https://icml.cc/media/icml-2022/Slides/16514_OuEYI6G.pdf)
- Different clustering models (finite mixture vs DP mixture, LDA vs Hierarchical DP) yield different 
  clustering results
- Which should one use? How should one choose?
- Random partition models define a distribution over partitions of a set
- Two common, important assumptions:
  - Exchangeability: Relabeling the clusters doesn't affect the probability
  - Projectivity: Marginalizing out index is the same as if index never existed
    e.g. $$p(\{\{1, 2 \}\}) = p(\{\{1, 2\}, \{ 3 \} \}) + p(\{ \{ 1, 2, 3 \} \})$$
- Under exchangeability, the PMF can be written as a Exchangeability Probability Partition Function
- Gibbs partition: $$\Pi_n \sim Gibbs_{[n]}(V, W)$$ if $$p(n_1, ..., n_k) = V_{n,k} \prod_{j=1}^k W_{n_j}$$
  - The important property to note here is that the PMF depends only on the sizes of the clusters W_{n_j}
- Fixed partition 


## Tractable Uncertainty for Structure Learning
###  Benjie Wang, Matthew Wicker, Marta Kwiatkowska

- [Slides](https://icml.cc/media/icml-2022/Slides/18118_xfL5Ti2.pdf)
- Bayesian Structure Learning: learn directed acyclic graphs (DAGs)
- How to design good approximations over DAGs?
  - Sufficiently expressive
  - Tractable to answer queries of interest
- Sum Product Networks
  - Leaf nodes: simple base distribution $$L(X)$$
  - Product nodes: Factorize distribution $$P(X) = C_1(X) \times C_2(X)$$
  - Mix nodes: $$S(X) = \sum_j \phi_j C_j(X)$$
- Introduce OrderSPNs
  - Track the order of children in product nodes
  - Can interpret the topological ordering as ordering over nodes in DAG