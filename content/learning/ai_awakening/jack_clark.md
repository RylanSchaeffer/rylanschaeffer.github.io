# AI Awakening
## Implications for the Economy & Society

Speaker(s): [Jack Clark](https://jack-clark.net/about/)

For other speakers, [pop up a level](../ai_awakening.html).

## Comments

- Lots of new papers every day on ArXiv
- Research is moving quickly
- Scaling laws are everywhere
- Better to look at these models as _aliens_ than as _tools_
- Think about AIs as a tool that you _grow_


## Q&A

> Question: You spoke about scaling laws. Does the cost of inference/serving not impose an upper limit on how practically large the models can become?

Answer: The market will provide a pareto-optimal tradeoff between speed vs intelligence.

> Question: How is Washington attending to AI advances? 

Answer: Washington felt that they dropped the ball on social media. They are trying to get ahead of the curve this time.
Ted Cruz has already been ranting about "Woke AI."

> Question: Can we poll Language Models (LMs) to get a proxy of what Americans think politically and socially?

Answer: Previously, a few years ago, researchers explored this and found models were calibrated. Follow-up work also supported
this. This lowers the barriers to using LMs for polling and political analysis.

> Question: How representative are LLMs of the political spectrum?

Answer: Models may be biased towards the political spectrum of certain people. There are two possible futures,
depending on how steerable the models are. If the model can be prompted to role-play as a particular demographic,
then it's still useful. If it can't be steered in a particular way, then using it might be much harder.

> Question: I'm hearing you talk about the benefits of personality differences with LLMs, whereas performance
> improvements suggest a single model is the best way forward. With the ways policy & financial incentives are
> structured, what direction do you think we'll move in?

Answer: Dr. Fei Fei Li is involved in a project regarding whether we should build a national compute cluster.
England wants to train an English LLM with English values.
Will capital markets in different geographies facilitate this?
Unclear currently. Having 1-2 big models open one up to fragility. Probably want many models to be more robust.

> Question: If an LLM possesses emotional knowledge that will harm a human, is it acceptable for the LLM 
> to lie to protect the human?

Answer: You'll probably want the thing that causes the last harm. Claude's answer is much more detailed
and nuanced: "In the end, there is no easy or universal answer here."


