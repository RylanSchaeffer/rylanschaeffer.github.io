# Diffusion Models

## Score-Based Diffusion Models

For a given probability distribution $$p(x; \theta)$$, the **score function** is defined as the
gradient of the log density:

$$s(x) := \nabla_x \log p(x; \theta)$$

More probable data can be generated by starting at some arbitrary $$x(0)$$ and then
performing gradient ascent on the score function:

$$x(t + dt) = x(t) + \alpha s(x)$$

How does one train a score-based diffusion model? 



### Score Matching

The Fisher divergence between two distributions $$p(x)$$ and $$q(x)$$ is defined as:

$$D_F(p, q) := \frac{1}{2} \mathbb{E}_{x \sim p} \Big[ \lvert \lvert \nabla_x \log p(x) - \nabla_x \log q(x) \lvert \lvert_2^2 \Big]$$

Score matching argues for minimizing the Fisher divergence between the true score function
and the score function learned by the model $$

One approach to training a score-based DM is to use __denoising score-matching__.
The idea is to add small perturbative noise to the data and train a network to remove
the noise.

#### Denoising Score Matching

Vincent 2010

#### Sliced Score Matching

TOOD: Song et al. 2019

### Noise Conditional Score Networks

## Noise Contrastive Estimation

TODO

https://deepgenerativemodels.github.io/assets/slides/cs236_lecture12.pdf


## Connections to Other Topics

- [Connection to Associative Memory](associative_memory/associative_memory_and_diffusion_models.html)