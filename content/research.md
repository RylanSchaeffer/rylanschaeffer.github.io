# Research

If you're interested in collaborating, please email me at rylanschaeffer@gmail.com
[following my instructions](research/collaborating_on_research.md). For those curious, I've posted a (work-in-progress)
[summary of my research approach](research/research_philosophy.md).



This page is ~chronologically ordered. See my [Google Scholar](https://scholar.google.com/citations?user=6tMEGz8AAAAJ&hl=en)
for a complementary view.

## Accepted

-----

[Is Model Collapse Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data.](research/2024_arxiv_is_model_collapse_inevitable/main.md) __Arxiv 2024__.


[Many-shot Jailbreaking.](research/2024_arxiv_many_shot_jailbreaking/main.md) __Arxiv 2024__.


Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks. Under Review.


Bridging Associative Memory and Probabilistic Modeling. Under Review.


Towards an Improved Understanding and Utilization of Maximum Manifold Capacity Representations. Under Review.


What Causes Polysemanticity?\\An Alternative Origin Story of Mixed Selectivity from Incidental Causes. Under Review.


Does Data Contamination Make a Difference? Insights from Intentionally Contaminating Pre-training Data for Language Models. Under Review.


[Are Emergent Abilities of Large Language Models a Mirage?](research/2023_neurips_llm_emergent_abilities_mirage/main.md) __NeurIPS 2023 (Outstanding Paper)__.


[DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.](research/2023_neurips_decoding_trust/main.md) __NeurIPS 2023 Benchmark Track__.


[Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells.](research/2023_neurips_ssl_gc/main.md) __NeurIPS 2023__.


[Divergence at the Interpolation Threshold: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle.](research/2023_neurips_workshop_double_descent/main.md) __NeurIPS 2023 Workshops: ATTRIB, Mathematics of Modern Machine Learning__.


[An Information-Theoretic Understanding of Maximum Manifold Capacity Representations.](research/2023_neurips_workshop_mmcr_infotheory/main.md) __NeurIPS Workshops: UniReps (Oral), InfoCog (Spotlight), NeurReps, SSL__.


[Associative Memory Under the Probabilistic Lens: Improved Transformers & Dynamic Memory Creation.](research/2023_neurips_workshop_infinite_associative_memory/main.md) __NeurIPS 2023 Workshop: Associative Memories & Hopfield Networks__.


[Testing Assumptions Underlying a Unified Theory for the Origin of Grid Cells.](research/2023_neurips_workshop_unified_theory_assumptions/main.md)  __NeurIPS 2023 Workshops: UniReps, NeurReps, AI4Science__.


Beyond Expectations: Model-Driven Amplification of Dataset Biases in Data Feedback Loops. __NeurIPS 2023 Workshop: Algorithmic Fairness through the Lens of Time__.


[Emergence of Sparse Representations from Noise.](research/2023_icml_noise_sparse_coding/main.md) __ICML 2023__.


[Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting.](research/2023_icml_workshop_logically_invalid_chain_of_thought/main.md) __ICML 2023 Workshop: Knowledge and Logical Reasoning in the Era of Data-driven Learning__.


[Deceptive Alignment Monitoring.](research/2023_icml_workshop_deceptive_alignment_monitoring/main.md) __ICML 2023 AdvML Workshop (Blue Sky Oral)__.


[FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation.](research/2023_icml_workshop_facade/main.md) __ICML 2023 AdvML Workshop__.


[No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit.](research/2022_neurips_no_free_lunch/main.md) __NeurIPS 2022__.


Streaming Inference for Infinite Non-Stationary Clustering. __CoLLAs 2022__.


[Streaming Inference for Infinite Latent Feature Models.](research/2022_icml_streaming_ibp/main.md) __ICML 2022__.


[No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit.](research/2022_icml_workshop_no_free_lunch/main.md) __ICML 2022 Workshop: AI for Science__.


[Streaming Inference for Infinite Non-Stationary Clustering.](research/2022_iclr_workshop_aloe/main.md) __ICLR 2022 Workshop: Agent Learning in Open Endedness__.


[An Algorithmic Theory of Metacognition in Minds and Machines.](research/2021_neurips_workshop_metacognition/main.html) __NeurIPS 2021 Workshop: Metacognition in the Age of AI__.


[Efficient Online Inference for Nonparametric Mixture Models.](research/2021_uai_streaming_crp/main.md) __UAI 2021__.


[Neural population dynamics for hierarchical inference in mice performing the International Brain Lab task.](research/2021_sfn_ibl/main.md) __Society for Neuroscience 2021__.


[Neural network model of amygdalar memory engram formation and function.](research/2021_cosyne_amygdalar_engram/main.md) __COSYNE 2021__.


[Reverse-engineering recurrent neural network solutions to a hierarchical inference task for mice.](research/2020_neurips_reverse_engineering/main.md) __NeurIPS 2020__.



## Under Review

-----

Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle. __Under Review at ICLR 2024 Blog Track__.
- [Tweeprint](https://twitter.com/RylanSchaeffer/status/1640762626987925505)

Brain-wide population codes for hierarchical inference in mice. __SfN 2024__.

Brain-wide representations of prior information in mouse decision-making.  __bioRxiv 2023__.

A Brain-Wide Map of Neural Activity during Complex Behaviour. __bioRxiv 2023__.


## Preprints

-----

[Disentangling Fact from Grid Cell Fiction in Trained Deep Path Integrators.](research/2023_biorxiv_disentangling_fact_from_grid_cell_fiction/main.md) __Biorxiv 2023__.

[Pretraining on the Test Set Is All You Need.](research/2023_arxiv_pretraining_on_test_set/main.md) __Arxiv 2023__.


## In Preparation

-----

An Information-Theoretic Understanding of Maximum Manifold Capacity Representations.

Associative Memory Under the Probabilistic Lens: Improved Transformers & Dynamic Memory Creation.

Testing Assumptions Underlying a Unified Theory for the Origin of Grid Cells.


## Class Projects

-----

Towards Unifying Smooth Neural Codes with Adversarially Robust Representations. 2019.
  - [Paper](research/2019_am226_smooth_neural_codes/paper.pdf)

## One Day

-----

Memory engrams perform nonparametric non-stationary latent state associative learning.

Recovering low dimensional, interpretable mechanistic models via Representations and Dynamics Distillation (RADD).




## Explanations of Others' Research

- [Neural Turing Machines](research/neural_turing_machine/main.html)
- [One-shot Learning with Memory-Augmented Neural Networks](research/one_shot_learning_with_memory_augmented_nn/main.html)
- [Neural Episodic Control](research/neural_episodic_control/main.html)
- [Elastic Weight Consolidation](research/elastic_weight_consolidation/main.html)
- [Early Visual Concept Learning with Unsupervised Deep Learning](research/early_visual_concept_learning/main.html)
